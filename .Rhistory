library(glmnet)
library(pROC)
set.seed(8365)
myvocab <- scan(file = "myvocab.txt", what = character())
auc_split <- c()
for(j in 1:5){
setwd(paste("C:/Users/kchir/OneDrive/Desktop/STAT 542/project_3_final/split_", j, sep=""))
#####################################
# Load libraries
# Load your vocabulary and training data
#####################################
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
train$review <- gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review, preprocessor = tolower, tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab, ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
#####################################
# Train a binary classification model
#####################################
fit1 = cv.glmnet(x = dtm_train,y = train$sentiment,alpha = 0.5,family='binomial')
#####################################
# Load test data, and
# Compute prediction
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,header = TRUE)
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,preprocessor = tolower,tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,ngram = c(1L, 2L)))
dtm_test = create_dtm(it_test, vectorizer)
output = predict(fit1, dtm_test, s=fit1$lambda.min, type = 'response')
#####################################
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predicted probs
#####################################
test$prediction = output
output = data.frame(test$id, test$prediction)
names(output) = c('id', 'prob')
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
# evaluation
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
auc_split[j]<-auc(roc_obj)
}
auc_split
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(e1071)
library(glmnet)
library(pROC)
set.seed(8365)
myvocab <- scan(file = "myvocab.txt", what = character())
auc_split <- c()
for(j in 1:5){
setwd(paste("C:/Users/kchir/OneDrive/Desktop/STAT 542/project_3_final/split_", j, sep=""))
#####################################
# Load libraries
# Load your vocabulary and training data
#####################################
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
train$review <- gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review, preprocessor = tolower, tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab, ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
#####################################
# Train a binary classification model
#####################################
fit1 = cv.glmnet(x = dtm_train,y = train$sentiment,alpha = 1,family='binomial')
#####################################
# Load test data, and
# Compute prediction
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,header = TRUE)
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,preprocessor = tolower,tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,ngram = c(1L, 2L)))
dtm_test = create_dtm(it_test, vectorizer)
output = predict(fit1, dtm_test, s=fit1$lambda.min, type = 'response')
#####################################
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predicted probs
#####################################
test$prediction = output
output = data.frame(test$id, test$prediction)
names(output) = c('id', 'prob')
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
# evaluation
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
auc_split[j]<-auc(roc_obj)
}
auc_split
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
library(e1071)
library(glmnet)
library(pROC)
set.seed(8365)
myvocab <- scan(file = "myvocab.txt", what = character())
auc_split <- c()
for(j in 1:5){
setwd(paste("C:/Users/kchir/OneDrive/Desktop/STAT 542/project_3_final/split_", j, sep=""))
#####################################
# Load libraries
# Load your vocabulary and training data
#####################################
train <- read.table("train.tsv", stringsAsFactors = FALSE,
header = TRUE)
train$review <- gsub('<.*?>', ' ', train$review)
it_train = itoken(train$review, preprocessor = tolower, tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab, ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)
#####################################
# Train a binary classification model
#####################################
fit1 = cv.glmnet(x = dtm_train,y = train$sentiment,alpha = 0,family='binomial')
#####################################
# Load test data, and
# Compute prediction
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,header = TRUE)
test$review <- gsub('<.*?>', ' ', test$review)
it_test = itoken(test$review,preprocessor = tolower,tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab,ngram = c(1L, 2L)))
dtm_test = create_dtm(it_test, vectorizer)
output = predict(fit1, dtm_test, s=fit1$lambda.min, type = 'response')
#####################################
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predicted probs
#####################################
test$prediction = output
output = data.frame(test$id, test$prediction)
names(output) = c('id', 'prob')
write.table(output, file = "mysubmission.txt",
row.names = FALSE, sep='\t')
# evaluation
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
auc_split[j]<-auc(roc_obj)
}
auc_split
library(pROC)
source("mymain.R")
library(pROC)
source("mymain.R")
getwd
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(astsa)
library(astsa)
library(astsa)
library(xts)
library(forecast)
df <- read.csv("mycsvfile_WeeklySales", header = TRUE)
df <- read.csv("mycsvfile_WeeklySales", header = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(recommenderlab)
library(recommenderlab)
library(recommenderlab)
library(Matrix)
myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'),
sep = ':',
colClasses = c('integer', 'NULL'),
header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
ratings$Timestamp = NULL
set.seed(100)
train.id = sample(nrow(ratings), floor(nrow(ratings)) * 0.8)
train = ratings[train.id, ]
head(train)
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(data.table)
library(reshape2)
myurl = "https://liangfgithub.github.io/MovieData/"
movies = readLines(paste0(myurl, 'movies.dat?raw=true'))
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)
# convert accented characters
movies$Title[73]
movies$Title = iconv(movies$Title, "latin1", "UTF-8")
movies$Title[73]
# extract year
movies$Year = as.numeric(unlist(
lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(data.table)
library(reshape2)
myurl = "https://liangfgithub.github.io/MovieData/"
# use colClasses = 'NULL' to skip columns
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'),
sep = ':',
colClasses = c('integer', 'NULL'),
header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
movies = readLines(paste0(myurl, 'movies.dat?raw=true'))
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)
# convert accented characters
movies$Title[73]
movies$Title = iconv(movies$Title, "latin1", "UTF-8")
movies$Title[73]
# extract year
movies$Year = as.numeric(unlist(
lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
users = read.csv(paste0(myurl, 'users.dat?raw=true'),
sep = ':', header = FALSE)
users = users[, -c(2,4,6,8)] # skip columns
colnames(users) = c('UserID', 'Gender', 'Age', 'Occupation', 'Zip-code')
dim(users)
length(unique(ratings$UserID))
dim(movies)
length(unique(ratings$MovieID))
movies_not_rated = movies %>%
filter(!(MovieID %in% ratings$MovieID))
dim(movies_not_rated)
tmp = data.frame(Rating = 1:5,
freq = as.vector(table(ratings$Rating)/nrow(ratings)))
ggplot(data = tmp, aes(x = Rating, y = freq)) +
geom_bar(stat="identity", fill = 'steelblue', width = 0.6) +
geom_text(aes(label=round(freq, dig=2)),
vjust=1.6, color="white", size=3.5) +
theme_minimal()
tmp = ratings %>%
group_by(UserID) %>%
summarize(ratings_per_user = n())
summary(tmp$ratings_per_user)
stem(tmp$ratings_per_user)
sum(tmp$ratings_per_user > 500)
sort(tmp$ratings_per_user[tmp$ratings_per_user>1300])
tmp %>%
ggplot(aes(ratings_per_user)) +
geom_bar(fill = "steelblue") + coord_cartesian(c(20, 500))
tmp = tmp %>% full_join(users, by = 'UserID')
tmp = ratings %>%
group_by(MovieID) %>%
summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>%
inner_join(movies, by = 'MovieID')
summary(tmp$ratings_per_movie)
tmp %>%
filter(ratings_per_movie > 2000) %>%
arrange(desc = ratings_per_movie) %>%
select(c("Title", "ratings_per_movie")) %>%
print(n = 31)
tmp %>% ggplot(aes(ratings_per_movie)) +
geom_bar(fill = "steelblue", width = 1) + coord_cartesian(c(1,1500))
small_image_url = "https://liangfgithub.github.io/MovieImages/"
ratings %>%
group_by(MovieID) %>%
summarize(ratings_per_movie = n(),
ave_ratings = round(mean(Rating), dig=3)) %>%
inner_join(movies, by = 'MovieID') %>%
filter(ratings_per_movie > 1000) %>%
top_n(10, ave_ratings) %>%
mutate(Image = paste0('<img src="',
small_image_url,
MovieID,
'.jpg?raw=true"></img>')) %>%
select('Image', 'Title', 'ave_ratings') %>%
arrange(desc(-ave_ratings)) %>%
datatable(class = "nowrap hover row-border",
escape = FALSE,
options = list(dom = 't',
scrollX = TRUE, autoWidth = TRUE))
genres = as.data.frame(movies$Genres, stringsAsFactors=FALSE)
tmp = as.data.frame(tstrsplit(genres[,1], '[|]',
type.convert=TRUE),
stringsAsFactors=FALSE)
genre_list = c("Action", "Adventure", "Animation",
"Children's", "Comedy", "Crime",
"Documentary", "Drama", "Fantasy",
"Film-Noir", "Horror", "Musical",
"Mystery", "Romance", "Sci-Fi",
"Thriller", "War", "Western")
m = length(genre_list)
genre_matrix = matrix(0, nrow(movies), length(genre_list))
for(i in 1:nrow(tmp)){
genre_matrix[i,genre_list %in% tmp[i,]]=1
}
colnames(genre_matrix) = genre_list
remove("tmp", "genres")
data.frame(Genres = genre_list,
Freq = as.vector(colMeans(genre_matrix))) %>%
ggplot(aes(reorder(Genres, Freq), Freq, fill = Freq)) +
geom_bar(stat = "identity") +
geom_text(aes(label = round(Freq, dig=2)),
position = position_stack(vjust = 0.5),
color="white", size=3) +
coord_flip() +
scale_colour_brewer(palette="Set1") +
labs(y = 'Frequency', x = 'Genre')
tmp = ratings %>%
left_join(data.frame(MovieID = movies$MovieID, genre_matrix),
by = "MovieID") %>%
select(-c("UserID", "MovieID", "Rating", "Timestamp"))
data.frame(Genres = genre_list,
Popularity = as.vector(colMeans(tmp))) %>%
ggplot(aes(reorder(Genres, Popularity), Popularity, fill = Popularity)) +
geom_bar(stat = "identity") +
geom_text(aes(label = round(Popularity, dig=3)),
position = position_stack(vjust = 0.5),
color="white", size=3) +
coord_flip() +
labs(y = 'Popularity', x = 'Genre')
tmp = rowSums(genre_matrix)
summary(tmp)
movies[which(tmp==6), ]
movies[which(tmp==5), ]
# range(movies$Year) % 1919 to 2000
tmp = data.frame(Year = movies$Year, genre_matrix) %>%
group_by(Year) %>%
summarise_all(sum)
tmp[,-1] = apply(tmp[, -1], 2, cumsum)
tmp[,-1] = tmp[,-1]/sum(tmp[nrow(tmp), -1])
print(round(tmp[nrow(tmp),-1], dig=3))
tmp = reshape2::melt(tmp, id.vars="Year")
tmp %>%
ggplot(aes(Year, value, group = variable)) +
geom_area(aes(fill = variable)) +
geom_line(aes(group = variable), position = "stack")
library(ElemStatLearn)
library(ElemStatLearn)
install.packages('ElemStatLearn')
library(ElemStatLearn)
#library(ElemStatLearn)
train_df = library(zip.train)
#library(ElemStatLearn)
train_df = load(zip.train)
knitr::opts_chunk$set(echo = TRUE)
library(ElemStatLearn)
install.packages('ElemStatLearn')
install_version("ElemStatLearn", version = "2015.6.26.2", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("ElemStatLearn", version = "2015.6.26.2", repos = "http://cran.us.r-project.org")
require(devtools)
install_version("ElemStatLearn", version = "2015.6.26.2", repos = "http://cran.us.r-project.org")
library(ElemStatLearn)
load(zip.train)
library(ElemStatLearn)
dim(zip.train)
#load(zip.test)
library(ElemStatLearn)
dim(zip.train)
dim(zip.test)
train = as.data.frame(zip.train)
train
lda.model = lda(V~., data = train)
library(ElemStatLearn)
library(MASS)
dim(zip.train)
dim(zip.test)
train = as.data.frame(zip.train)
lda.model = lda(V~., data = train)
View(train)
lda.model = lda(V1~., data = train)
lda.model
test = as.data.frame(zip.test)
test
pred = predict(lda.model, test[, -1])
table(test[, 1], pred)
View(pred)
q1 = test[which(train$V1 %in% c(4)),]
View(q1)
q1 = test[which(train$V1 %in% c(4)),]
dim(q1)
View(test)
q1 = test[which(test$V1 %in% c(4)),]
dim(q1)
q2 = pred[which(pred$V1 %in% c(4)),]
lda.model = lda(V1~., data = train)
lda.model
View(lda.model)
pred = predict(lda.model, test[, -1])
table(test[, 1], pred)
pred = predict(lda.model, test[, -1])
table(test[, 1], pred)
dim(pred)
pred = predict(lda.model$xlevels, test[, -1])
View(pred)
pred = predict(lda.model$class, test[, -1])
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(data.table)
library(reshape2)
#loading the data
myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'),
sep = ':',
colClasses = c('integer', 'NULL'),
header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
# convert accented characters
movies$Title[73]
movies$Title = iconv(movies$Title, "latin1", "UTF-8")
movies$Title[73]
# extract the movie year
movies$Year = as.numeric(unlist(
lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
# Read the User Data
users = read.csv(paste0(myurl, 'users.dat?raw=true'),
sep = ':', header = FALSE)
users = users[, -c(2,4,6,8)] # skip columns
colnames(users) = c('UserID', 'Gender', 'Age', 'Occupation', 'Zip-code')
# metric 1: by popularity
popular_genre = function(gfrominput)
{
genre = ratings %>%
group_by(MovieID) %>%
#summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>%
summarize(ratings_per_movie = sum(Rating > 2)) %>%
inner_join(movies, by = 'MovieID') %>%
group_by(Genres) %>%
arrange(desc = ratings_per_movie)
#filter(ratings_per_movie > 1000) %>%
#top_n(5, ratings_per_movie)
genre = genre[grepl(gfrominput, genre$Genres, fixed = TRUE),]
#return(genere[1:2,])
nr =nrow(genre)
low=nr-5+1
return(genre[nr:low,])
}
# popular_genere = function(gfrominput)
# {
#   genere = ratings %>%
#     group_by(MovieID) %>%
#     summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>%
#     inner_join(movies, by = 'MovieID') %>%
#     group_by(Genres) %>%
#     filter(ratings_per_movie > 1000) %>%
#     top_n(2, ave_ratings)
#   genere = genere[grepl(gfrominput, genere$Genres, fixed = TRUE),]
#   return(genere)
# }
# metric 2: highly-rated
# small_image_url = "https://liangfgithub.github.io/MovieImages/"
# ratings_result = ratings %>%
#   group_by(MovieID) %>%
#   summarize(ratings_per_movie = n(),
#             ave_ratings = round(mean(Rating), dig=3)) %>%
#   inner_join(movies, by = 'MovieID') %>%
#   group_by(Genres) %>%
#   filter(ratings_per_movie > 1000) %>%
#   top_n(10, ave_ratings) %>%
#   mutate(Image = paste0('<img src="',
#                         small_image_url,
#                         MovieID,
#                         '.jpg?raw=true"></img>')) %>%
#   select( 'Title', 'ave_ratings',"MovieID","Genres") %>%
#
#   arrange(desc(-ave_ratings))
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(DT)
library(data.table)
library(reshape2)
#loading the data
myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'),
sep = ':',
colClasses = c('integer', 'NULL'),
header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
# convert accented characters
movies$Title[73]
#get ratings
myurl = "https://liangfgithub.github.io/MovieData/"
ratings = read.csv(paste0(myurl, 'ratings.dat?raw=true'),
sep = ':',
colClasses = c('integer', 'NULL'),
header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
# extract year
movies$Year = as.numeric(unlist(
lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
shiny::runApp('C:/Users/kchir/Downloads/project 4')
shiny::runApp()
